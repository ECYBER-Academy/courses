{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb9c6d1",
   "metadata": {},
   "source": [
    "# NLTK Workshop Exercises\n",
    "Using the UCI Sentiment Labelled Sentences Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path, sep='\\t', header=None, names=['sentence', 'label'])\n",
    "    return data\n",
    "\n",
    "# Load from Yelp (repeat for IMDb and Amazon as needed)\n",
    "data = load_data('yelp_labelled.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenization and Stopword Removal\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [t for t in tokens if t not in stop_words and t not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['sentence'].apply(preprocess)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Stemming and Lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['stemmed'] = data['tokens'].apply(lambda tokens: [stemmer.stem(w) for w in tokens])\n",
    "data['lemmatized'] = data['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(w) for w in tokens])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36587ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data['pos_tags'] = data['tokens'].apply(nltk.pos_tag)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05519a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Sentiment Analysis with VADER\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "data['vader_score'] = data['sentence'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf972bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Feature Extraction and Model Training\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['sentence'])\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3471f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7: Perform token frequency analysis for positive and negative sentences separately\n",
    "# Hint: Use NLTK's FreqDist and compare most common tokens\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "positive_tokens = [token for sent in df[df['label'] == 1]['tokens'] for token in sent]\n",
    "negative_tokens = [token for sent in df[df['label'] == 0]['tokens'] for token in sent]\n",
    "\n",
    "positive_freq = FreqDist(positive_tokens)\n",
    "negative_freq = FreqDist(negative_tokens)\n",
    "\n",
    "print(\"Most common words in positive sentences:\")\n",
    "print(positive_freq.most_common(10))\n",
    "\n",
    "print(\"\\nMost common words in negative sentences:\")\n",
    "print(negative_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ac8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8: Identify and visualize bigrams in the sentences\n",
    "# Hint: Use nltk.bigrams and FreqDist\n",
    "\n",
    "from nltk.util import bigrams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_bigrams = list(bigrams([token for sent in df['tokens'] for token in sent]))\n",
    "bigram_freq = FreqDist(all_bigrams)\n",
    "\n",
    "print(\"Most common bigrams:\")\n",
    "print(bigram_freq.most_common(10))\n",
    "\n",
    "# Plotting\n",
    "bigram_freq.plot(10, title=\"Top 10 Most Common Bigrams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439feb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9: Build a simple sentiment classifier using Naive Bayes\n",
    "# Hint: Use NLTK's NaiveBayesClassifier\n",
    "\n",
    "from nltk import NaiveBayesClassifier, classify\n",
    "\n",
    "# Feature extraction: simple word presence\n",
    "def extract_features(words):\n",
    "    return {word: True for word in words}\n",
    "\n",
    "features = [(extract_features(tokens), label) for tokens, label in zip(df['tokens'], df['label'])]\n",
    "\n",
    "# Split into training and test sets\n",
    "train_size = int(0.8 * len(features))\n",
    "train_set, test_set = features[:train_size], features[train_size:]\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "\n",
    "print(\"Naive Bayes Classifier Accuracy:\", accuracy)\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fbb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Join tokens into a single string for each sentiment\n",
    "positive_text = ' '.join(positive_tokens)\n",
    "negative_text = ' '.join(negative_tokens)\n",
    "\n",
    "# Generate word clouds\n",
    "positive_wc = WordCloud(width=600, height=400, background_color='white').generate(positive_text)\n",
    "negative_wc = WordCloud(width=600, height=400, background_color='black', colormap='Reds').generate(negative_text)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(positive_wc, interpolation='bilinear')\n",
    "plt.title('Positive Sentiment Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(negative_wc, interpolation='bilinear')\n",
    "plt.title('Negative Sentiment Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
