{
  "cells": [
    {
      "source": "# Python pandas tutorial: The ultimate guide for beginners\n\nAre you ready to begin your pandas journey? Here’s a step-by-step guide on how to get started.",
      "metadata": {},
      "id": "cbf86638-e05f-460b-90d2-06dbc96a1c37",
      "cell_type": "markdown"
    },
    {
      "source": "pandas is arguably the most important Python package for data analysis. With over 100 million downloads per month, it is the de facto standard package for data manipulation and exploratory data analysis. Its ability to read from and write to an extensive list of formats makes it a versatile tool for data science practitioners. Its data manipulation functions make it a highly accessible and practical tool for aggregating, analyzing, and cleaning data. \n\nIn our blog post on [how to learn pandas](https://www.datacamp.com/blog/how-to-learn-pandas), we discussed the learning path you may take to master this package. This beginner-friendly tutorial will cover all the basic concepts and illustrate pandas' different functions. You can also check out our course on [pandas Foundations](https://www.datacamp.com/courses/data-manipulation-with-pandas) for further details. \n\nThis article is aimed at beginners with basic knowledge of Python and no prior experience with pandas to help you get started.\n\n## What is pandas?\n\npandas is a data manipulation package in Python for tabular data. That is, data in the form of rows and columns, also known as DataFrames. Intuitively, you can think of a DataFrame as an Excel sheet. \n\npandas’ functionality includes data transformations, like [sorting rows](https://www.datacamp.com/tutorial/pandas-sort-values) and taking subsets, to calculating summary statistics such as the mean, reshaping DataFrames, and joining DataFrames together. pandas works well with other popular Python data science packages, often called the PyData ecosystem, including\n\n- [**NumPy**](https://www.datacamp.com/tutorial/python-numpy-tutorial) for numerical computing\n- [**Matplotlib**](https://www.datacamp.com/tutorial/matplotlib-tutorial-python), [**Seaborn**](https://www.datacamp.com/tutorial/seaborn-python-tutorial), [**Plotly**](https://www.datacamp.com/courses/introduction-to-data-visualization-with-plotly-in-python), and other data visualization packages\n- [**scikit-learn**](https://www.datacamp.com/tutorial/machine-learning-python) for machine learning\n\n\n## What is pandas used for?\n\npandas is used throughout the data analysis workflow. With pandas, you can:\n\n- Import datasets from databases, spreadsheets, comma-separated values (CSV) files, and more.\n- Clean datasets, for example, by dealing with missing values.\n- Tidy datasets by reshaping their structure into a suitable format for analysis.\n- Aggregate data by calculating summary statistics such as the mean of columns, correlation between them, and more.\n- Visualize datasets and uncover insights.\n\n\npandas also contains functionality for time series analysis and analyzing text data.\n\n## Key benefits of the pandas package\n\nUndoubtedly, pandas is a powerful data manipulation tool packaged with several benefits, including:\n\n- **Made for Python:** Python is the world's most popular language for machine learning and data science.\n- **Less verbose per unit operations:** Code written in pandas is less verbose, requiring fewer lines of code to get the desired output.  \n- **Intuitive view of data:** pandas offers exceptionally intuitive data representation that facilitates easier data understanding and analysis. \n- **Extensive feature set:** It supports an extensive set of operations from exploratory data analysis, dealing with missing values, calculating statistics, visualizing univariate and bivariate data, and much more.\n- **Works with large data:** pandas handles large data sets with ease. It offers speed and efficiency while working with datasets of the order of millions of records and hundreds of columns, depending on the machine.\n\n\n## How to install pandas?\n\nBefore delving into its functionality, let us first install pandas. You can avoid this step by [registering for a free DataCamp account](https://www.datacamp.com/users/sign_up) and using [DataLab](https://www.datacamp.com/datalab), DataLab cloud-based IDE that comes with pandas (alongside the top python data science packages) pre-installed.",
      "metadata": {},
      "id": "1e1afda3-683a-4bdc-b78f-fb5bbef79f32",
      "cell_type": "markdown"
    },
    {
      "source": "### Install pandas\n\nInstalling pandas is straightforward; just use the `pip install` command in your terminal.",
      "metadata": {},
      "id": "52318479-f6b0-47d6-bd00-5f9c1307844a",
      "cell_type": "markdown"
    },
    {
      "source": "pip install pandas",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": null,
        "lastExecutedAt": null,
        "lastExecutedByKernel": null,
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": null,
        "outputsMetadata": { "0": { "height": 290, "type": "stream" } }
      },
      "id": "70206433-6782-4778-86af-5e642fa5b3ad",
      "cell_type": "code",
      "execution_count": 108,
      "outputs": []
    },
    {
      "source": "## Importing data in pandas\n\nTo begin working with pandas, import the pandas Python package as shown below. When importing pandas, the most common alias for pandas is `pd`.",
      "metadata": {},
      "id": "ac15113a-7d8c-49c6-9387-a6805d5bd0ba",
      "cell_type": "markdown"
    },
    {
      "source": "import pandas as pd",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590921336,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "import pandas as pd"
      },
      "id": "7b26d44e-5b0d-4125-9bb5-5cd1d90b91fa",
      "cell_type": "code",
      "execution_count": 109,
      "outputs": []
    },
    {
      "source": "### Importing CSV files\n\nUse `read_csv()` with the path to the CSV file to read a comma-separated values file (see our [tutorial on importing data with read_csv()](https://www.datacamp.com/tutorial/pandas-read-csv) for more detail).",
      "metadata": {},
      "id": "fe465297-3f84-44c8-b2d3-51791ed0e844",
      "cell_type": "markdown"
    },
    {
      "source": "df = pd.read_csv(\"diabetes.csv\")",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 47,
        "lastExecutedAt": 1722590921384,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df = pd.read_csv(\"diabetes.csv\")"
      },
      "id": "4ee43269-4fd4-4034-8b2f-a4db6d3ff95b",
      "cell_type": "code",
      "execution_count": 110,
      "outputs": []
    },
    {
      "source": "This read operation loads the CSV file `diabetes.csv` to generate a pandas Dataframe object `df`. Throughout this tutorial, you'll see how to manipulate such DataFrame objects. \n\n### Importing text files\n\nReading text files is similar to CSV files. The only nuance is that you need to specify a separator with the `sep` argument, as shown below. The separator argument refers to the symbol used to separate rows in a DataFrame. Comma (`sep = \",\"`), whitespace(`sep = \"\\s\"`), tab (`sep = \"\\t\"`), and colon(`sep = \":\"`) are the commonly used separators. Additionaly, a regular expression is accepted. Here `\"r\\s+\"` represents at least one white space character.",
      "metadata": {},
      "id": "b1d9f4ba-ebad-4aea-a2c6-368f444248e6",
      "cell_type": "markdown"
    },
    {
      "source": "df = pd.read_csv(\"diabetes.txt\", sep=\"r\\s+\") ",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590921432,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df = pd.read_csv(\"diabetes.txt\", sep=\"r\\s+\") "
      },
      "id": "64fc13e9-8da7-4f62-bf33-17ad5b396f36",
      "cell_type": "code",
      "execution_count": 111,
      "outputs": []
    },
    {
      "source": "### Importing Excel files (single sheet)\n\nReading excel files (both XLS and XLSX) is as easy as the `read_excel()` function, using the file path as an input.",
      "metadata": {},
      "id": "025db288-25ba-4f60-80ba-ccecb1d2de3b",
      "cell_type": "markdown"
    },
    {
      "source": "df = pd.read_excel('diabetes.xlsx')",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 136,
        "lastExecutedAt": 1722590921569,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df = pd.read_excel('diabetes.xlsx')"
      },
      "id": "37342915-ea31-416f-b30f-9a4c117f8e0b",
      "cell_type": "code",
      "execution_count": 112,
      "outputs": []
    },
    {
      "source": "You can also specify other arguments, such as `header` for to specify which row becomes the DataFrame's header. It has a default value of `0`, which denotes the first row as headers or column names. You can also specify column names as a list in the `names` argument. The `index_col` (default is `None`) argument can be used if the file contains a row index.\n\n_**Note:** In a pandas DataFrame or Series, the index is an identifier that points to the location of a row or column in a pandas DataFrame. In a nutshell, the index labels the row or column of a DataFrame and lets you access a specific row or column by using its index (you will see this later on). A DataFrame’s row index can be a range (e.g., 0 to 303), a time series (dates or timestamps), a unique identifier (e.g., _`employee_ID`_ in an _`employees`_ table), or other types of data. For columns, it's usually a string (denoting the column name)._\n\n### Importing Excel files (multiple sheets)\n\nReading Excel files with multiple sheets is not that different. You just need to specify one additional argument, `sheet_name`, where you can either pass a string for the sheet name or an integer for the sheet position (note that Python uses 0-indexing, where the first sheet can be accessed with `sheet_name = 0`)",
      "metadata": {},
      "id": "1d3e18ae-9ac7-4775-8b8c-35603d605278",
      "cell_type": "markdown"
    },
    {
      "source": "# Extracting the second sheet since Python uses 0-indexing\ndf = pd.read_excel('diabetes_multi.xlsx', sheet_name=1)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 173,
        "lastExecutedAt": 1722590921743,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# Extracting the second sheet since Python uses 0-indexing\ndf = pd.read_excel('diabetes_multi.xlsx', sheet_name=1)"
      },
      "id": "cc9f34b4-e016-41ac-bf5f-c61575403af4",
      "cell_type": "code",
      "execution_count": 113,
      "outputs": []
    },
    {
      "source": "### Importing JSON file\n\nSimilar to the `read_csv()` function, you can use `read_json()` for JSON file types with the JSON file name as the argument (for more detail read [this tutorial on importing JSON and HTML data into pandas](https://www.datacamp.com/tutorial/importing-data-into-pandas)). The below code reads a JSON file from disk and creates a DataFrame object `df`.",
      "metadata": {},
      "id": "6ee0b9f3-f4fc-4740-8c32-9f23e435ddfa",
      "cell_type": "markdown"
    },
    {
      "source": "df = pd.read_json(\"diabetes.json\")",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590921792,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df = pd.read_json(\"diabetes.json\")"
      },
      "id": "ecd5b6a0-7240-4afd-9328-345bc7895578",
      "cell_type": "code",
      "execution_count": 114,
      "outputs": []
    },
    {
      "source": "If you want to learn more about importing data with pandas, check out this [cheat sheet](https://www.datacamp.com/cheat-sheet/importing-data-in-python-cheat-sheet) on importing various file types with Python. \n\n## Outputting data in pandas\n\nJust as pandas can import data from various file types, it also allows you to export data into various formats. This happens especially when data is transformed using pandas and needs to be saved locally on your machine. Below is how to output pandas DataFrames into various formats.\n\n### Outputting a DataFrame into a CSV file\n\nA pandas DataFrame (here we are using `df`) is saved as a CSV file using the `.to_csv()` method. The arguments include the filename with path and `index` – where `index = True` implies writing the DataFrame’s index.",
      "metadata": {},
      "id": "ba84dd5d-2b33-4e3a-b03d-17eabde6e0a4",
      "cell_type": "markdown"
    },
    {
      "source": "df.to_csv(\"diabetes_out.csv\", index=False)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 47,
        "lastExecutedAt": 1722590921840,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.to_csv(\"diabetes_out.csv\", index=False)"
      },
      "id": "851b8784-6c31-4e7c-8b33-abca3a3cf175",
      "cell_type": "code",
      "execution_count": 115,
      "outputs": []
    },
    {
      "source": "### Outputting a DataFrame into a JSON file\n\nExport DataFrame object into a JSON file by calling the `.to_json()` method.",
      "metadata": {},
      "id": "766e963a-17cb-425a-bdf5-0fce66bc005f",
      "cell_type": "markdown"
    },
    {
      "source": "df.to_json(\"diabetes_out.json\")",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 47,
        "lastExecutedAt": 1722590921888,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.to_json(\"diabetes_out.json\")"
      },
      "id": "dea58f6d-66cc-490e-942f-05983cb5b0b3",
      "cell_type": "code",
      "execution_count": 116,
      "outputs": []
    },
    {
      "source": "_**Note:** A JSON file stores a tabular object like a DataFrame as a key-value pair. Thus you would observe repeating column headers in a JSON file._\n\n### Outputting a DataFrame into a text file\n\nAs with writing DataFrames to CSV files, you can call `.to_csv()`. The only differences are that the output file format is in `.txt`, and you need to specify a separator using the `sep` argument.",
      "metadata": {},
      "id": "419e17b4-da93-447f-aa74-b3a4f605a082",
      "cell_type": "markdown"
    },
    {
      "source": "df.to_csv('diabetes_out.txt', header=df.columns, index=None, sep=' ')",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590921940,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.to_csv('diabetes_out.txt', header=df.columns, index=None, sep=' ')"
      },
      "id": "6d6b9951-f822-4092-9f49-659d3320cb6a",
      "cell_type": "code",
      "execution_count": 117,
      "outputs": []
    },
    {
      "source": "### Outputting a DataFrame into an Excel file\n\nCall `.to_excel()` from the DataFrame object to save it as a `“.xls”` or `“.xlsx”` file.",
      "metadata": {},
      "id": "80c26eb0-8ed7-4ee4-ba3e-78bedf57362a",
      "cell_type": "markdown"
    },
    {
      "source": "df.to_excel(\"diabetes_out.xlsx\", index=False)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 140,
        "lastExecutedAt": 1722590922081,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.to_excel(\"diabetes_out.xlsx\", index=False)"
      },
      "id": "09da128e-b5aa-42ae-a6d2-b8ec0ab7a596",
      "cell_type": "code",
      "execution_count": 118,
      "outputs": []
    },
    {
      "source": "## Viewing and understanding DataFrames using pandas \n\nAfter reading tabular data as a DataFrame, you would need to have a glimpse of the data. You can either view a small sample of the dataset or a summary of the data in the form of summary statistics.\n\n### How to view data using `.head()` and `.tail()`\n\nYou can view the first few or last few rows of a DataFrame using the `.head()` or `.tail()` methods, respectively. You can specify the number of rows through the `n` argument (the default value is 5).",
      "metadata": {},
      "id": "b488e85d-281c-4b74-82b2-6e9a1714b236",
      "cell_type": "markdown"
    },
    {
      "source": "df.head() # The first five rows of the DataFrame",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 63,
        "lastExecutedAt": 1722590922145,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.head() # The first five rows of the DataFrame",
        "outputsMetadata": { "0": { "height": 196, "type": "dataFrame" } }
      },
      "id": "3b31ddfc-eee9-48c7-8114-7a896ef23497",
      "cell_type": "code",
      "execution_count": 119,
      "outputs": []
    },
    {
      "source": "df.tail(n = 10) # The last 10 rows of the DataFrame",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590922193,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.tail(n = 10) # The last 10 rows of the DataFrame",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "1fcf2f64-3127-4736-b9da-706911008a0c",
      "cell_type": "code",
      "execution_count": 120,
      "outputs": []
    },
    {
      "source": "### Understanding data using `.describe()`\n\nThe `.describe()` method prints the summary statistics of all numeric columns, such as count, mean, standard deviation, range, and quartiles of numeric columns.",
      "metadata": {},
      "id": "6dae554c-eeed-4970-bfdf-764a8568e361",
      "cell_type": "markdown"
    },
    {
      "source": "df.describe()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590922249,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.describe()",
        "outputsMetadata": { "0": { "height": 271, "type": "dataFrame" } }
      },
      "id": "4663644a-f631-43f5-a76f-6f911179a759",
      "cell_type": "code",
      "execution_count": 121,
      "outputs": []
    },
    {
      "source": "It gives a quick look at the scale, skew, and range of numeric data.\n\nYou can also modify the quartiles using the `percentiles` argument. Here, for example, we’re looking at the 30%, 50%, and 70% percentiles of the numeric columns in DataFrame `df`.",
      "metadata": {},
      "id": "6a695b05-a902-4054-a275-d4d06b2b2309",
      "cell_type": "markdown"
    },
    {
      "source": "df.describe(percentiles=[0.3, 0.5, 0.7])",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590922301,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.describe(percentiles=[0.3, 0.5, 0.7])",
        "outputsMetadata": { "0": { "height": 271, "type": "dataFrame" } }
      },
      "id": "69d2b96e-55fc-42ff-8a4c-f141b4774215",
      "cell_type": "code",
      "execution_count": 122,
      "outputs": []
    },
    {
      "source": "You can also isolate specific data types in your summary output by using the `include` argument. Here, for example, we’re only summarizing the columns with the `integer` data type.",
      "metadata": {},
      "id": "f7375ded-f607-48d3-aeb5-f400f510e59e",
      "cell_type": "markdown"
    },
    {
      "source": "df.describe(include=[int])",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590922357,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.describe(include=[int])",
        "outputsMetadata": { "0": { "height": 271, "type": "dataFrame" } }
      },
      "id": "9f17da5e-35f5-4d74-acb2-699f9ed52c02",
      "cell_type": "code",
      "execution_count": 123,
      "outputs": []
    },
    {
      "source": "Similarly, you might want to exclude certain data types using `exclude` argument.",
      "metadata": {},
      "id": "71134f74-6b0d-44c7-a5cb-7dd65fdc9a50",
      "cell_type": "markdown"
    },
    {
      "source": "df.describe(exclude=[int])",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590922409,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.describe(exclude=[int])",
        "outputsMetadata": { "0": { "height": 271, "type": "dataFrame" } }
      },
      "id": "a9d7e260-c49c-4d8c-9f48-91841e22a7bc",
      "cell_type": "code",
      "execution_count": 124,
      "outputs": []
    },
    {
      "source": "Often, practitioners find it easy to view such statistics by transposing them with the `.T` attribute.",
      "metadata": {},
      "id": "c9112407-89d9-4ffa-89b2-0f363a456efb",
      "cell_type": "markdown"
    },
    {
      "source": "df.describe().T",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590922461,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.describe().T",
        "outputsMetadata": { "0": { "height": 296, "type": "dataFrame" } }
      },
      "id": "120830a1-d92d-4aca-a97a-0f4a37d5e953",
      "cell_type": "code",
      "execution_count": 125,
      "outputs": []
    },
    {
      "source": "For more on describing DataFrames, check out the following [cheat sheet](https://www.datacamp.com/cheat-sheet/pandas-cheat-sheet-data-wrangling-in-python).",
      "metadata": {},
      "id": "e8df7db1-1ad0-4096-82e3-1dc843acdf5a",
      "cell_type": "markdown"
    },
    {
      "source": "### Understanding data using `.info()`\n\nThe `.info()` method is a quick way to look at the data types, missing values, and data size of a DataFrame. Here, we’re setting the `show_counts` argument to `True`, which gives a few over the total non-missing values in each column. We’re also setting `memory_usage` to `True`, which shows the total memory usage of the DataFrame elements. When `verbose` is set to `True`, it prints the full summary from `.info()`.",
      "metadata": {},
      "id": "359fc34b-a98c-4d10-a738-c4690c337307",
      "cell_type": "markdown"
    },
    {
      "source": "df.info(show_counts=True, memory_usage=True, verbose=True)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 57,
        "lastExecutedAt": 1722590922518,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.info(show_counts=True, memory_usage=True, verbose=True)",
        "outputsMetadata": { "0": { "height": 353, "type": "stream" } }
      },
      "id": "f143d708-00a0-47d4-b9b5-da89b709e503",
      "cell_type": "code",
      "execution_count": 126,
      "outputs": []
    },
    {
      "source": "### Understanding your data using `.shape`\n\nThe number of rows and columns of a DataFrame can be identified using the `.shape` attribute of the DataFrame. It returns a tuple (row, column) and can be indexed to get only rows, and only columns count as output.",
      "metadata": {},
      "id": "130f88b1-6686-4713-95ac-5abf8f0ed106",
      "cell_type": "markdown"
    },
    {
      "source": "df.shape # Get the number of rows and columns",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 55,
        "lastExecutedAt": 1722590922573,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.shape # Get the number of rows and columns"
      },
      "id": "2f1bb274-ff03-493e-8207-10265b56fc65",
      "cell_type": "code",
      "execution_count": 127,
      "outputs": []
    },
    {
      "source": "df.shape[0] # Get the number of rows only",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590922629,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.shape[0] # Get the number of rows only"
      },
      "cell_type": "code",
      "id": "0d6a10bf-be57-4d28-8188-69ad07bc62c0",
      "outputs": [],
      "execution_count": 128
    },
    {
      "source": "df.shape[1] # Get the number of columns only",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590922685,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.shape[1] # Get the number of columns only"
      },
      "cell_type": "code",
      "id": "42ee9fed-ef40-4810-bcb0-f29ef8ef12b3",
      "outputs": [],
      "execution_count": 129
    },
    {
      "source": "### Get all columns and column names\n\nCalling the `.columns` attribute of a DataFrame object returns the column names in the form of an `Index` object. As a reminder, a pandas index is the address/label of the row or column.",
      "metadata": {},
      "id": "8d48e010-db5f-412d-b56d-6d1f2d7ea1de",
      "cell_type": "markdown"
    },
    {
      "source": "df.columns",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590922737,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.columns"
      },
      "id": "72ec8f81-839c-45a7-9707-11539a84ead2",
      "cell_type": "code",
      "execution_count": 130,
      "outputs": []
    },
    {
      "source": "It can be converted to a list using a `list()` function.",
      "metadata": {},
      "id": "cac49b52-44d5-4d39-bf20-596db58aa79a",
      "cell_type": "markdown"
    },
    {
      "source": "list(df.columns)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590922789,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "list(df.columns)"
      },
      "id": "1ded4a65-599f-457b-ae4b-0810a2225ded",
      "cell_type": "code",
      "execution_count": 131,
      "outputs": []
    },
    {
      "source": "### Checking for missing values in pandas with `.isnull()`\n\nThe sample DataFrame does not have any missing values. Let's introduce a few to make things interesting. The `.copy()` method makes a copy of the original DataFrame. This is done to ensure that any changes to the copy don’t reflect in the original DataFrame. Using `.loc` (to be discussed later), you can set rows two to five of the `Pregnancies` column to `NaN` values, which denote missing values.",
      "metadata": {},
      "id": "cb91de9a-bda5-41c2-bd89-4f53db863bbd",
      "cell_type": "markdown"
    },
    {
      "source": "df2 = df.copy()\ndf2.loc[2:5,'Pregnancies'] = None\ndf2.head(7)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590922841,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2 = df.copy()\ndf2.loc[2:5,'Pregnancies'] = None\ndf2.head(7)",
        "outputsMetadata": { "0": { "height": 246, "type": "dataFrame" } }
      },
      "id": "732eef24-f22b-4e99-ae38-fbe757114a2a",
      "cell_type": "code",
      "execution_count": 132,
      "outputs": []
    },
    {
      "source": "_You can see, that now rows 2 to 5 are `NaN`_\n\nYou can check whether each element in a DataFrame is missing using the `.isnull()` method.",
      "metadata": {},
      "id": "b1b5dc65-f81b-4b2b-8125-710e65d78f26",
      "cell_type": "markdown"
    },
    {
      "source": "df2.isnull().head(7)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590922897,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.isnull().head(7)",
        "outputsMetadata": { "0": { "height": 246, "type": "dataFrame" } }
      },
      "id": "ea30e012-53e2-48bc-9e41-4bf0e8c80bf9",
      "cell_type": "code",
      "execution_count": 133,
      "outputs": []
    },
    {
      "source": "Given it's often more useful to know how much missing data you have, you can combine `.isnull()` with `.sum()` to count the number of nulls in each column.",
      "metadata": {},
      "id": "45128814-a4cd-4c9c-9f03-a4b3247aa76a",
      "cell_type": "markdown"
    },
    {
      "source": "df2.isnull().sum()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590922953,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.isnull().sum()",
        "outputsMetadata": { "0": { "height": 296, "type": "dataFrame" } }
      },
      "id": "2d733c03-707a-4b8b-9af9-c2e41daa6c7b",
      "cell_type": "code",
      "execution_count": 134,
      "outputs": []
    },
    {
      "source": "You can also do a double sum to get the total number of nulls in the DataFrame.",
      "metadata": {},
      "id": "d4338822-560a-423d-95d7-f053dc87e1d9",
      "cell_type": "markdown"
    },
    {
      "source": "df2.isnull().sum().sum()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590923005,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.isnull().sum().sum()"
      },
      "id": "81d09dde-c0c4-4c14-93db-0a5d6c6fd8e9",
      "cell_type": "code",
      "execution_count": 135,
      "outputs": []
    },
    {
      "source": "## Slicing and Extracting Data in pandas\n\nThe pandas package offers several ways to subset, filter, and isolate data in your DataFrames. Here, we'll see the most common ways.\n\n### Isolating one column using `[ ]` \n\nYou can isolate a single column using a square bracket `[ ]` with a column name in it. The output is a pandas `Series` object. A pandas Series is a one-dimensional array containing data of any type, including integer, float, string, boolean, python objects, etc. A DataFrame is comprised of many series that act as columns.",
      "metadata": {},
      "id": "110238bb-7381-49da-bbd7-029205db4307",
      "cell_type": "markdown"
    },
    {
      "source": "df['Outcome']",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590923057,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df['Outcome']",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "526d9d97-222d-4a40-a38d-09943a125784",
      "cell_type": "code",
      "execution_count": 136,
      "outputs": []
    },
    {
      "source": "### Isolating two or more columns using `[[ ]]` \n\nYou can also provide a list of column names inside the square brackets to fetch more than one column. Here, square brackets are used in two different ways. We use the outer square brackets to indicate a subset of a DataFrame, and the inner square brackets to create a list.",
      "metadata": {},
      "id": "63536bfa-bd79-4c0e-8d1e-34535cb3ed24",
      "cell_type": "markdown"
    },
    {
      "source": "df[['Pregnancies', 'Outcome']]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590923113,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df[['Pregnancies', 'Outcome']]",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "a79ab967-fa81-401e-8d4c-10abe6a2d145",
      "cell_type": "code",
      "execution_count": 137,
      "outputs": []
    },
    {
      "source": "### Isolating one row using `[ ]` \n\nA single row can be fetched by passing in a boolean series with one `True` value. In the example below, the second row with `index = 1` is returned. Here, `.index` returns the row labels of the DataFrame, and the comparison turns that into a Boolean one-dimensional array.",
      "metadata": {},
      "id": "ad7aed36-e7d8-46b8-81fb-59098cfe6ce5",
      "cell_type": "markdown"
    },
    {
      "source": "df[df.index==1]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590923169,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df[df.index==1]",
        "outputsMetadata": { "0": { "height": 96, "type": "dataFrame" } }
      },
      "id": "5ea9e0c3-0308-4a22-a6f1-ac8e883cf8c8",
      "cell_type": "code",
      "execution_count": 138,
      "outputs": []
    },
    {
      "source": "### Isolating two or more rows using `[ ]` \n\nSimilarly, two or more rows can be returned using the `.isin()` method instead of a `==` operator.",
      "metadata": {},
      "id": "a62c783e-86cb-4558-873b-fa74e8a460f2",
      "cell_type": "markdown"
    },
    {
      "source": "df[df.index.isin(range(2,10))]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590923217,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df[df.index.isin(range(2,10))]",
        "outputsMetadata": { "0": { "height": 271, "type": "dataFrame" } }
      },
      "id": "01205863-0781-4748-be80-29bf2d4c61db",
      "cell_type": "code",
      "execution_count": 139,
      "outputs": []
    },
    {
      "source": "### Using `.loc[]` and `.iloc[]` to fetch rows\n\nYou can fetch specific rows by labels or conditions using `.loc[]` and `.iloc[]` (\"location\" and \"integer location\"). `.loc[]` uses a label to point to a row, column or cell, whereas `.iloc[]` uses the numeric position. To understand the difference between the two, let’s modify the index of `df2` created earlier.",
      "metadata": {},
      "id": "5b9025f4-ba10-42aa-adb2-8c0528aa8f70",
      "cell_type": "markdown"
    },
    {
      "source": "df2.index = range(1,769)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 47,
        "lastExecutedAt": 1722590923264,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.index = range(1,769)"
      },
      "id": "4d29388f-0b27-42dd-bff3-024e8e2dc0ea",
      "cell_type": "code",
      "execution_count": 140,
      "outputs": []
    },
    {
      "source": "The below example returns a pandas `Series` instead of a DataFrame. The `1` represents the row index (label), whereas the `1` in .`iloc[]` is the row position (first row).",
      "metadata": {},
      "id": "f13d390c-e6d8-4535-9916-b9b170df7131",
      "cell_type": "markdown"
    },
    {
      "source": "df2.loc[1]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 53,
        "lastExecutedAt": 1722590923317,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.loc[1]",
        "outputsMetadata": { "0": { "height": 296, "type": "dataFrame" } }
      },
      "id": "6b1b4246-e0a5-4dae-b113-d3fed97051df",
      "cell_type": "code",
      "execution_count": 141,
      "outputs": []
    },
    {
      "source": "df2.iloc[1]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590923369,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.iloc[1]",
        "outputsMetadata": { "0": { "height": 296, "type": "dataFrame" } }
      },
      "id": "11f6f259-52bf-4912-aec5-afa9de6a7128",
      "cell_type": "code",
      "execution_count": 142,
      "outputs": []
    },
    {
      "source": "You can also fetch multiple rows by providing a range in square brackets.",
      "metadata": {},
      "id": "f974d27f-7d26-4757-ab35-1c15987f4bdd",
      "cell_type": "markdown"
    },
    {
      "source": "df2.loc[100:110]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590923417,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.loc[100:110]",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "fabce25f-791b-4556-8281-b49711f14e08",
      "cell_type": "code",
      "execution_count": 143,
      "outputs": []
    },
    {
      "source": "df2.iloc[100:110]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590923469,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.iloc[100:110]",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "292be0b5-21a0-4adf-acab-cb39db85a5de",
      "cell_type": "code",
      "execution_count": 144,
      "outputs": []
    },
    {
      "source": "You can also subset with `.loc[]` and `.iloc[]` by using a list instead of a range.",
      "metadata": {},
      "id": "60842c7d-3742-4267-b773-9595b2bc2362",
      "cell_type": "markdown"
    },
    {
      "source": "df2.loc[[100, 200, 300]]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590923521,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.loc[[100, 200, 300]]",
        "outputsMetadata": { "0": { "height": 146, "type": "dataFrame" } }
      },
      "id": "65d38def-1973-4e0f-b459-26b49cc68b22",
      "cell_type": "code",
      "execution_count": 145,
      "outputs": []
    },
    {
      "source": "df2.iloc[[100, 200, 300]]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 49,
        "lastExecutedAt": 1722590923570,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.iloc[[100, 200, 300]]",
        "outputsMetadata": { "0": { "height": 146, "type": "dataFrame" } }
      },
      "id": "9b0efe82-115e-486f-bce1-301192ce07ee",
      "cell_type": "code",
      "execution_count": 146,
      "outputs": []
    },
    {
      "source": "You can also select specific columns along with rows. This is where `.iloc[]` is different from `.loc[]` – it requires column location and not column labels.",
      "metadata": {},
      "id": "64d03f1e-83fd-4038-8770-09e84565ef92",
      "cell_type": "markdown"
    },
    {
      "source": "df2.loc[100:110, ['Pregnancies', 'Glucose', 'BloodPressure']]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 63,
        "lastExecutedAt": 1722590923633,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.loc[100:110, ['Pregnancies', 'Glucose', 'BloodPressure']]",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "b07e9a49-3a4b-441c-acb1-21f0de95e178",
      "cell_type": "code",
      "execution_count": 147,
      "outputs": []
    },
    {
      "source": "df2.iloc[100:110, :3]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 55,
        "lastExecutedAt": 1722590923688,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.iloc[100:110, :3]",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "1fe76bc1-0188-445f-a3c6-c4c0651e435c",
      "cell_type": "code",
      "execution_count": 148,
      "outputs": []
    },
    {
      "source": "For faster workflows, you can pass in the starting index of a row as a range.",
      "metadata": {},
      "id": "68e16ccf-159c-413b-9b5f-acda37251a4d",
      "cell_type": "markdown"
    },
    {
      "source": "df2.loc[760:, ['Pregnancies', 'Glucose', 'BloodPressure']]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 56,
        "lastExecutedAt": 1722590923744,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.loc[760:, ['Pregnancies', 'Glucose', 'BloodPressure']]",
        "outputsMetadata": { "0": { "height": 296, "type": "dataFrame" } }
      },
      "id": "39f05115-fa5a-477d-994d-628a85c74c6c",
      "cell_type": "code",
      "execution_count": 149,
      "outputs": []
    },
    {
      "source": "df2.iloc[760:, :3]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 57,
        "lastExecutedAt": 1722590923801,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.iloc[760:, :3]",
        "outputsMetadata": { "0": { "height": 271, "type": "dataFrame" } }
      },
      "id": "2343f407-d64d-48ee-a355-157662df0509",
      "cell_type": "code",
      "execution_count": 150,
      "outputs": []
    },
    {
      "source": "You can update/modify certain values by using the assignment operator `=`",
      "metadata": {},
      "id": "e6541555-a372-44be-81f0-9c99e5ed1803",
      "cell_type": "markdown"
    },
    {
      "source": "df2.loc[df2['Age'] == 81, 'Age'] = 80",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 47,
        "lastExecutedAt": 1722590923848,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.loc[df2['Age'] == 81, 'Age'] = 80"
      },
      "id": "20f79cb2-e3c4-4c03-bfe5-7908c668612a",
      "cell_type": "code",
      "execution_count": 151,
      "outputs": []
    },
    {
      "source": "### Conditional slicing (that fits certain conditions)\n\npandas lets you filter data by conditions over row/column values. For example, the below code selects the row where Blood Pressure is exactly 122. Here, we are isolating rows using the brackets `[ ]` as seen in previous sections. However, instead of inputting row indices or column names, we are inputting a condition where the column `BloodPressure` is equal to 122. We denote this condition using `df.BloodPressure == 122`.",
      "metadata": {},
      "id": "2077d54e-30c8-4202-a394-2add47feea36",
      "cell_type": "markdown"
    },
    {
      "source": "df[df.BloodPressure == 122]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590923896,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df[df.BloodPressure == 122]",
        "outputsMetadata": { "0": { "height": 96, "type": "dataFrame" } }
      },
      "id": "909d706f-8de0-401d-a471-deb0562702b0",
      "cell_type": "code",
      "execution_count": 152,
      "outputs": []
    },
    {
      "source": "The below example fetched all rows where `Outcome` is 1. Here `df.Outcome` selects that column, `df.Outcome == 1` returns a Series of Boolean values determining which `Outcomes` are equal to 1, then `[]` takes a subset of `df` where that Boolean Series is `True`.",
      "metadata": {},
      "id": "9e849843-a38b-40ef-b462-3aea4d1f18a1",
      "cell_type": "markdown"
    },
    {
      "source": "df[df.Outcome == 1]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590923944,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df[df.Outcome == 1]",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "a037c097-ac6a-46e5-8b76-f807d2b64369",
      "cell_type": "code",
      "execution_count": 153,
      "outputs": []
    },
    {
      "source": "You can use a `>` operator to draw comparisons. The below code fetches `Pregnancies`, `Glucose`, and `BloodPressure` for all records with `BloodPressure` greater than 100.",
      "metadata": {},
      "id": "a10f9e12-f5df-419e-ae62-6912a11981b9",
      "cell_type": "markdown"
    },
    {
      "source": "df.loc[df['BloodPressure'] > 100, ['Pregnancies', 'Glucose', 'BloodPressure']]",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590923992,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.loc[df['BloodPressure'] > 100, ['Pregnancies', 'Glucose', 'BloodPressure']]",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "6ef6e1d9-8ba7-4c44-a5f5-6a51eca86de3",
      "cell_type": "code",
      "execution_count": 154,
      "outputs": []
    },
    {
      "source": "## Cleaning data using pandas \n\nData cleaning is one of the most common tasks in data science. pandas lets you preprocess data for any use, including but not limited to training machine learning and deep learning models. Let’s use the DataFrame `df2` from earlier, having four missing values, to illustrate a few data cleaning use cases. As a reminder, here's how you can see how many missing values are in a DataFrame.",
      "metadata": {},
      "id": "2b5e46f9-d088-4aab-abf3-c79d9501d04e",
      "cell_type": "markdown"
    },
    {
      "source": "df2.isnull().sum()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590924044,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2.isnull().sum()",
        "outputsMetadata": { "0": { "height": 296, "type": "dataFrame" } }
      },
      "id": "f81155b2-7d1d-4e94-980d-bc410b76a54c",
      "cell_type": "code",
      "execution_count": 155,
      "outputs": []
    },
    {
      "source": "### Dealing with missing data technique #1: Dropping missing values\n\nOne way to deal with missing data is to drop it. This is particularly useful in cases where you have plenty of data and losing a small portion won’t impact the downstream analysis. You can use a `.dropna()` method as shown below. Here, we are saving the results from `.dropna()` into a DataFrame `df3`.",
      "metadata": {},
      "id": "45e66210-8e11-4d75-a4eb-3bdddff72f53",
      "cell_type": "markdown"
    },
    {
      "source": "df3 = df2.copy()\ndf3 = df3.dropna()\ndf3.shape\n# Result is 4 rows less than df2",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924092,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df3 = df2.copy()\ndf3 = df3.dropna()\ndf3.shape\n# Result is 4 rows less than df2"
      },
      "id": "d130505b-3efa-4934-ae69-b7a16b2e1eb0",
      "cell_type": "code",
      "execution_count": 156,
      "outputs": []
    },
    {
      "source": "The `axis` argument lets you specify whether you are dropping rows, or [columns](https://www.datacamp.com/tutorial/pandas-drop-column), with missing values. The default `axis` removes the rows containing NaNs. Use `axis = 1` to remove the columns with one or more NaN values. Also, notice how we are using the argument `inplace=True` which lets you skip saving the output of `.dropna()` into a new DataFrame.",
      "metadata": {},
      "id": "ff1a5b23-2072-477c-bc2e-5f1cc8c1fcaf",
      "cell_type": "markdown"
    },
    {
      "source": "df3 = df2.copy()\ndf3.dropna(inplace=True, axis=1)\ndf3.head()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 60,
        "lastExecutedAt": 1722590924152,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df3 = df2.copy()\ndf3.dropna(inplace=True, axis=1)\ndf3.head()",
        "outputsMetadata": { "0": { "height": 196, "type": "dataFrame" } }
      },
      "id": "3b4aec90-348b-44c9-a039-fc0d8be450cf",
      "cell_type": "code",
      "execution_count": 157,
      "outputs": []
    },
    {
      "source": "You can also drop both rows and columns with missing values by setting the `how` argument to `'all'`",
      "metadata": {},
      "id": "2e709ef3-16fe-4133-9675-fadd087905a7",
      "cell_type": "markdown"
    },
    {
      "source": "df3 = df2.copy()\ndf3.dropna(inplace=True, how='all')",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590924204,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df3 = df2.copy()\ndf3.dropna(inplace=True, how='all')"
      },
      "id": "a41c38d9-cbc1-4798-987c-0b49cbbea3eb",
      "cell_type": "code",
      "execution_count": 158,
      "outputs": []
    },
    {
      "source": "### Dealing with missing data technique #2: Replacing missing values\n\nInstead of dropping, replacing missing values with a summary statistic or a specific value (depending on the use case) maybe the best way to go. For example, if there is one missing row from a temperature column denoting temperatures throughout the days of the week, replacing that missing value with the average temperature of that week may be more effective than dropping values completely. You can replace the missing data with the row, or column mean using the code below.",
      "metadata": {},
      "id": "03412db0-c166-4633-b96e-267c2749cf51",
      "cell_type": "markdown"
    },
    {
      "source": "df3 = df2.copy()\n# Get the mean of Pregnancies\nmean_value = df3['Pregnancies'].mean()\n# Fill missing values using .fillna()\ndf3 = df3.fillna(mean_value)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924252,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df3 = df2.copy()\n# Get the mean of Pregnancies\nmean_value = df3['Pregnancies'].mean()\n# Fill missing values using .fillna()\ndf3 = df3.fillna(mean_value)"
      },
      "id": "cf7bd271-73ca-49fb-b936-c0f7f13fe5f5",
      "cell_type": "code",
      "execution_count": 159,
      "outputs": []
    },
    {
      "source": "### Dealing with Duplicate Data\n\nLet's add some duplicates to the original data to learn how to eliminate duplicates in a DataFrame. Here, we are using the `.concat()` method to concatenate the rows of the `df2` DataFrame to the `df2` DataFrame, adding perfect duplicates of every row in `df2`.",
      "metadata": {},
      "id": "4de612e8-47b2-4134-9078-3a6cd6789478",
      "cell_type": "markdown"
    },
    {
      "source": "df3 = pd.concat([df2, df2])\ndf3.shape",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 47,
        "lastExecutedAt": 1722590924300,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df3 = pd.concat([df2, df2])\ndf3.shape"
      },
      "id": "b0566f7f-61cb-4047-ab7f-8ff7dc94baac",
      "cell_type": "code",
      "execution_count": 160,
      "outputs": []
    },
    {
      "source": "You can remove all duplicate rows (default) from the DataFrame [using `.drop_duplicates()` method](https://www.datacamp.com/tutorial/pandas-drop-duplicates).",
      "metadata": {},
      "id": "8f0490f1-c6d3-4702-a392-04dc6f0dc91c",
      "cell_type": "markdown"
    },
    {
      "source": "df3 = df3.drop_duplicates()\ndf3.shape",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924348,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df3 = df3.drop_duplicates()\ndf3.shape"
      },
      "id": "2e2189e8-bda3-4c8e-99ff-6c0adb244f4f",
      "cell_type": "code",
      "execution_count": 161,
      "outputs": []
    },
    {
      "source": "### Renaming columns\n\nA common data cleaning task is renaming columns. With the `.rename()` method, you can use `columns` as an argument to rename specific columns. The below code shows the dictionary for mapping old and new column names.",
      "metadata": {},
      "id": "87e66fd1-cc36-423b-9b25-4df783f625c0",
      "cell_type": "markdown"
    },
    {
      "source": "df3.rename(columns = {'DiabetesPedigreeFunction':'DPF'}, inplace = True)\ndf3.head()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924396,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df3.rename(columns = {'DiabetesPedigreeFunction':'DPF'}, inplace = True)\ndf3.head()",
        "outputsMetadata": { "0": { "height": 196, "type": "dataFrame" } }
      },
      "id": "7bddb5ac-83b8-4784-9812-930803c4c85e",
      "cell_type": "code",
      "execution_count": 162,
      "outputs": []
    },
    {
      "source": "You can also directly assign column names as a list to the DataFrame.",
      "metadata": {},
      "id": "d5ae0551-bd70-4b28-be6c-2f793dd90ae6",
      "cell_type": "markdown"
    },
    {
      "source": "df3.columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DPF', 'Age', 'Outcome', 'STF']\ndf3.head()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924444,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df3.columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DPF', 'Age', 'Outcome', 'STF']\ndf3.head()",
        "outputsMetadata": { "0": { "height": 196, "type": "dataFrame" } }
      },
      "id": "9b949dd8-8694-444f-a05c-b94dfac8c592",
      "cell_type": "code",
      "execution_count": 163,
      "outputs": []
    },
    {
      "source": "For more on data cleaning, and for easier, more predictable data cleaning workflows, check out the following checklist, which provides you with a comprehensive set of common [data cleaning tasks](https://www.datacamp.com/blog/infographic-data-cleaning-checklist). \n\n## Data analysis in pandas\n\nThe main value proposition of pandas lies in its quick data analysis functionality. In this section, we'll focus on a set of analysis techniques you can use in pandas.\n\n### Summary operators (mean, mode, median)\n\nAs you saw earlier, you can get the mean of each column value using the `.mean()` method.",
      "metadata": {},
      "id": "8eb5ef69-794b-4d4f-b86d-c5e55d5c02c5",
      "cell_type": "markdown"
    },
    {
      "source": "df.mean()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 51,
        "lastExecutedAt": 1722590924496,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.mean()",
        "outputsMetadata": { "0": { "height": 296, "type": "dataFrame" } }
      },
      "id": "b0d7769f-cba8-4e58-b70c-3ef0528e5018",
      "cell_type": "code",
      "execution_count": 164,
      "outputs": []
    },
    {
      "source": "A mode can be computed similarly using the `.mode()` method.",
      "metadata": {},
      "id": "e5a5754f-fb1d-4fe5-88bc-1c78175419ce",
      "cell_type": "markdown"
    },
    {
      "source": "df.mode()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590924548,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.mode()",
        "outputsMetadata": { "0": { "height": 121, "type": "dataFrame" } }
      },
      "id": "7437556f-2d74-47f1-9ef9-54235170a3ba",
      "cell_type": "code",
      "execution_count": 165,
      "outputs": []
    },
    {
      "source": "Similarly, the median of each column is computed with the `.median()` method",
      "metadata": {},
      "id": "e527c2ce-3ba9-4ed7-8a2a-92b8414cd731",
      "cell_type": "markdown"
    },
    {
      "source": "df.median()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924596,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.median()",
        "outputsMetadata": { "0": { "height": 296, "type": "dataFrame" } }
      },
      "id": "85b65379-9dc0-42df-b65d-038280ddf5f6",
      "cell_type": "code",
      "execution_count": 166,
      "outputs": []
    },
    {
      "source": "### Create new columns based on existing columns \n\npandas provides fast and efficient computation by combining two or more columns like scalar variables. The below code divides each value in the column `Glucose` with the corresponding value in the `Insulin` column to compute a new column named `Glucose_Insulin_Ratio`.",
      "metadata": {},
      "id": "57065002-8b82-4353-8b72-2290dddaaec7",
      "cell_type": "markdown"
    },
    {
      "source": "df2['Glucose_Insulin_Ratio'] = df2['Glucose']/df2['Insulin']\ndf2.head()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924644,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df2['Glucose_Insulin_Ratio'] = df2['Glucose']/df2['Insulin']\ndf2.head()",
        "outputsMetadata": { "0": { "height": 196, "type": "dataFrame" } }
      },
      "id": "171c21f9-69c6-4bb3-9db1-1662c5efa376",
      "cell_type": "code",
      "execution_count": 167,
      "outputs": []
    },
    {
      "source": "### Counting using `.value_counts()`\n\nOften times you'll work with categorical values, and you'll want to count the number of observations each category has in a column. Category values can be counted using the .`value_counts()` methods. Here, for example, we are counting the number of observations where `Outcome` is diabetic (1) and the number of observations where the `Outcome` is non-diabetic (0).",
      "metadata": {},
      "id": "55b377d0-b483-4622-9b09-730519dc61bf",
      "cell_type": "markdown"
    },
    {
      "source": "df['Outcome'].value_counts()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 44,
        "lastExecutedAt": 1722590924688,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df['Outcome'].value_counts()",
        "outputsMetadata": { "0": { "height": 121, "type": "dataFrame" } }
      },
      "id": "41bad661-9b60-4777-a924-2ba35bc39c2e",
      "cell_type": "code",
      "execution_count": 168,
      "outputs": []
    },
    {
      "source": "Adding the `normalize` argument returns proportions instead of absolute counts.",
      "metadata": {},
      "id": "6bcb36ec-3bb0-48e8-93dd-d96b6a7debb4",
      "cell_type": "markdown"
    },
    {
      "source": "df['Outcome'].value_counts(normalize=True)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924736,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df['Outcome'].value_counts(normalize=True)",
        "outputsMetadata": { "0": { "height": 121, "type": "dataFrame" } }
      },
      "id": "32846c5f-8b1e-4dbf-82f8-008232053a88",
      "cell_type": "code",
      "execution_count": 169,
      "outputs": []
    },
    {
      "source": "Turn off automatic sorting of results using `sort` argument (`True` by default). The default sorting is based on the counts in descending order.",
      "metadata": {},
      "id": "5119779f-25c0-4b65-86bb-4bad6ea6ba20",
      "cell_type": "markdown"
    },
    {
      "source": "df['Outcome'].value_counts(sort=False)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 52,
        "lastExecutedAt": 1722590924788,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df['Outcome'].value_counts(sort=False)",
        "outputsMetadata": { "0": { "height": 121, "type": "dataFrame" } }
      },
      "id": "8b24ab2a-71a6-46f4-9f08-8adc743423c3",
      "cell_type": "code",
      "execution_count": 170,
      "outputs": []
    },
    {
      "source": "You can also apply `.value_counts()` to a DataFrame object and specific columns within it instead of just a column. Here, for example, we are applying `value_counts()` on `df` with the subset argument, which takes in a list of columns.",
      "metadata": {},
      "id": "23730c76-6097-4b14-84ab-517f7a932a1e",
      "cell_type": "markdown"
    },
    {
      "source": "df.value_counts(subset=['Pregnancies', 'Outcome'])",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924836,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.value_counts(subset=['Pregnancies', 'Outcome'])",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "6490399f-ff82-4d04-af89-0b0d408b1e44",
      "cell_type": "code",
      "execution_count": 171,
      "outputs": []
    },
    {
      "source": "### Aggregating data with `.groupby()` in pandas\n\npandas lets you aggregate values by grouping them by specific column values. You can do that by combining the `.groupby()` method with a summary method of your choice. The below code displays the mean of each of the numeric columns grouped by `Outcome`.",
      "metadata": {},
      "id": "70713874-d418-482e-bf0c-bef2392051ca",
      "cell_type": "markdown"
    },
    {
      "source": "df.groupby('Outcome').mean()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924884,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.groupby('Outcome').mean()",
        "outputsMetadata": { "0": { "height": 121, "type": "dataFrame" } }
      },
      "id": "8159ad95-68bb-4ab1-8624-cf3dd5cb25e7",
      "cell_type": "code",
      "execution_count": 172,
      "outputs": []
    },
    {
      "source": "`.groupby()` enables grouping by more than one column by passing a list of column names, as shown below.",
      "metadata": {},
      "id": "ea8203e4-2a48-478c-a932-a9df79e2753a",
      "cell_type": "markdown"
    },
    {
      "source": "df.groupby(['Pregnancies', 'Outcome']).mean()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924932,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.groupby(['Pregnancies', 'Outcome']).mean()",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "f97696d0-89bc-4010-8c20-ab19e11f25c2",
      "cell_type": "code",
      "execution_count": 173,
      "outputs": []
    },
    {
      "source": "Any summary method can be used alongside `.groupby()`, including `.min()`, `.max()`, `.mean()`, `.median()`, `.sum()`, `.mode()`, and more.\n\n### Pivot tables \n\npandas also enables you to calculate summary statistics as pivot tables. This makes it easy to draw conclusions based on a combination of variables. The below code picks the rows as unique values of `Pregnancies`, the column values are the unique values of `Outcome`, and the cells contain the average value of `BMI` in the corresponding group.\n\nFor example, for `Pregnancies = 5` and `Outcome = 0`, the average BMI turns out to be 31.1.",
      "metadata": {},
      "id": "6fd84e7f-ef58-4c35-86a7-e3f1567c70ab",
      "cell_type": "markdown"
    },
    {
      "source": "import numpy as np\n\npd.pivot_table(df, values=\"BMI\", index='Pregnancies', \n               columns=['Outcome'], aggfunc=np.mean)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 48,
        "lastExecutedAt": 1722590924980,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "import numpy as np\n\npd.pivot_table(df, values=\"BMI\", index='Pregnancies', \n               columns=['Outcome'], aggfunc=np.mean)",
        "outputsMetadata": { "0": { "height": 321, "type": "dataFrame" } }
      },
      "id": "437d0051-8c1e-4740-adc8-407a67208a20",
      "cell_type": "code",
      "execution_count": 174,
      "outputs": []
    },
    {
      "source": "## Data visualization in pandas\n\npandas provides convenience wrappers to `Matplotlib` plotting functions to make it easy to visualize your DataFrames. Below, you'll see how to do common data visualizations using pandas.\n\n### Line plots in pandas\n\npandas enables you to chart out the relationships among variables using line plots. Below is a line plot of BMI and Glucose versus the row index.",
      "metadata": {},
      "id": "97d262ed-2c7b-4119-8a20-b0be42b76fb4",
      "cell_type": "markdown"
    },
    {
      "source": "df[['BMI', 'Glucose']].plot.line()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 357,
        "lastExecutedAt": 1722590925337,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df[['BMI', 'Glucose']].plot.line()"
      },
      "id": "f908c5ca-8ad9-4774-9c9b-c21f27f53f2e",
      "cell_type": "code",
      "execution_count": 175,
      "outputs": []
    },
    {
      "source": "You can select the choice of colors by using the color argument.",
      "metadata": {},
      "id": "f8b5f25e-7b76-4210-a899-34b8c1ad6b2a",
      "cell_type": "markdown"
    },
    {
      "source": "df[['BMI', 'Glucose']].plot.line(figsize=(20, 10), \n                                 color={\"BMI\": \"red\", \"Glucose\": \"blue\"})",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 713,
        "lastExecutedAt": 1722590926050,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df[['BMI', 'Glucose']].plot.line(figsize=(20, 10), \n                                 color={\"BMI\": \"red\", \"Glucose\": \"blue\"})"
      },
      "id": "8560d4d4-213c-4c1a-89de-559cf544d956",
      "cell_type": "code",
      "execution_count": 176,
      "outputs": []
    },
    {
      "source": "All the columns of `df` can also be plotted on different scales and axes by using the `subplots` argument.",
      "metadata": {},
      "id": "2e70e4a7-7402-4129-b76c-87e6ba27f065",
      "cell_type": "markdown"
    },
    {
      "source": "df.plot.line(subplots=True)",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 1379,
        "lastExecutedAt": 1722590927429,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.plot.line(subplots=True)"
      },
      "id": "09167237-e7c4-4c05-920f-96995c4315c8",
      "cell_type": "code",
      "execution_count": 177,
      "outputs": []
    },
    {
      "source": "### Bar plots in pandas\n\nFor discrete columns, you can use a bar plot over the category counts to visualize their distribution. The variable `Outcome` with binary values is visualized below.",
      "metadata": {},
      "id": "408bdaac-5f16-4402-9ded-499cc0758424",
      "cell_type": "markdown"
    },
    {
      "source": "df['Outcome'].value_counts().plot.bar()",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 151,
        "lastExecutedAt": 1722590927580,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df['Outcome'].value_counts().plot.bar()"
      },
      "id": "9d60888b-8d6a-4b2e-ac7b-f1bd4df6d60f",
      "cell_type": "code",
      "execution_count": 178,
      "outputs": []
    },
    {
      "source": "### Box plots in pandas\n\nThe quartile distribution of continuous variables can be visualized using a boxplot. The code below lets you create a boxplot with pandas.",
      "metadata": {},
      "id": "12272c2a-7377-4c38-9d0c-e8c6bd00dfbe",
      "cell_type": "markdown"
    },
    {
      "source": "df.boxplot(column=['BMI'], by='Outcome')",
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 204,
        "lastExecutedAt": 1722590927784,
        "lastExecutedByKernel": "b5137a9b-901c-4387-813f-ee5ac4b7169c",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "df.boxplot(column=['BMI'], by='Outcome')"
      },
      "id": "d8e89f7e-88f4-478d-8105-9695389120ad",
      "cell_type": "code",
      "execution_count": 179,
      "outputs": []
    },
    {
      "source": "## Learn more about pandas\n\nThe tutorial above scratches the surface of what's possible with pandas. Whether analyzing data, visualizing it, filtering, or aggregating it, pandas provides an incredibly rich feature set that lets you accelerate any data workflow. Moreover, by combining pandas with other data science packages, you'll be able to create interactive dashboards, create predictive models using machine learning, automate data workflows, and more. Check out the resources below to accelerate your pandas learning journey:\n\n- [[Cheat Sheets](https://www.datacamp.com/cheat-sheet/category/python)] A plethora of Python and [pandas cheat sheets](https://www.datacamp.com/cheat-sheet/pandas-cheat-sheet-for-data-science-in-python) to reference throughout your learning \n- [[Live trainings](https://www.datacamp.com/resources)] Check out our free live-code-along sessions, many of which leverage pandas\n- [[More tutorials](https://www.datacamp.com/tutorial/category/python)] Check out our remaining tutorials on pandas and the PyData ecosystem, including [how to implement moving averages in pandas](https://www.datacamp.com/tutorial/moving-averages-in-pandas) and [using the pandas .apply() method](https://www.datacamp.com/tutorial/pandas-apply)",
      "metadata": {},
      "id": "9fb4f953-73c3-461c-b1d4-8ecbebd9cf8f",
      "cell_type": "markdown"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": { "name": "ipython", "version": 3 },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "editor": "DataLab"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
